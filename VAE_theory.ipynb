{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405fa2c5",
   "metadata": {},
   "source": [
    "\n",
    "1. The Training Phase (Learning)Input: We feed a batch of real images ($x$) into the Encoder.\n",
    "\n",
    "Probabilistic Encoding: The Encoder does not output a fixed code. Instead, it predicts the parameters ($\\mu$ and $\\sigma$) of a distribution for that specific image.\n",
    "Reparameterization Trick: To allow backpropagation, we sample a latent vector $z$ using the formula $z = \\mu + \\sigma \\cdot \\epsilon$ (where $\\epsilon$ is random noise).Reconstruction: \n",
    "\n",
    "The Decoder takes $z$ and attempts to recreate the original pixels, outputting probabilities for each pixel (0 to 1).\n",
    "The Prior Check: We simultaneously pass $z$ to the Prior (GMM). \n",
    "\n",
    "The Prior acts as a \"judge,\" calculating the likelihood $p(z)$ to see if this point falls into a valid cluster in the latent space.The Loss Calculation: We calculate the Negative ELBO by combining two opposing forces:\n",
    "\n",
    "Maximize Reconstruction (RE): \"Make the output look like the input.\"Minimize Divergence (KL): \"Make the latent code $z$ fit the shape of the Prior's clusters.\"Optimization: Backpropagation adjusts the weights of the Encoder, Decoder, and Prior to balance these two goals.\n",
    "\n",
    "\n",
    "2. The Generation Phase (Creating)Sampling: We discard the Encoder and the real images. We ask the Prior to select a random point $z$ from its learned mixture of distributions.Decoding: The Decoder takes this \"imagined\" $z$ and generates a brand-new image from scratch.You have successfully implemented a system that learns to compress reality (Encoding) and dream up new realities (Generation)!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
